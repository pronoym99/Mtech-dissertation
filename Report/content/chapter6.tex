\chapter{Conclusions} \label{chapter6}

The various recommendations being made in NEWS shows were successfully detected (within tolerable error limits across a large size of input data), extracted and analysed for anomalies in the concerned process.  The input to the program script was simply the video file itself and the telecast date (which was reflected as it is in the last column). The summarised data was available to the end-user in either an excel file or CSV file for further analysis. The final summarised data contained important fields like the \textbf{Name of the analyst}, the \textbf{share/stock} regarding which recommendation was being made: its \textbf{name} as well as its \textbf{listing symbol} in the appropriate exchange, the \textbf{actual recommendation} being made (buy, sell or hold), the \textbf{target price} and the associated \textbf{stop loss}.  A total of eight NEWS shows were processed in this manner wherein only a single NEWS show was skipped due to the lack of recommendations being made in the video dataset available to us. \par

As stated earlier in chapter \ref{chapter4}, the deployment of the above ML/DL models was parallelly done onto two separate systems, namely the HPE Ezmeral MLOPs server for single usage low-frequency requests and on a separate Linux virtual machine for large free-running batch processing workflows which would keep on running a single script continuously on a large dataset. \par

I got to learn a lot of things regarding machine learning, deep learning for computer vision, functioning of OCR engines, MLOPs deployment in different environments and as well as different guidelines which are to be followed for gathering a video dataset (e.g. sampling at FPS rate, annotating a dataset, partitioning the dataset properly etc.) and processing it accordingly for further analysis. Overall it turned out to be an entire knowledge enriching experience spanning a multitude of domains.
