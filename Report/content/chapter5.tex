\chapter{Results and discussions} \label{chapter5}

The following subsections shall discuss in detail, a variety of important comparisons which affect the project work, a brief overview of the methodologies learned, deviations from the ideal case (if any) and a creator’s road map for implementation of a similar project and other moderately or less important details. It also has a section for the work which is being currently done at SEBI to improve the existing system. The initial two sections deal with metrics of accuracy and execution time across two different bases for comparison while the section that follows discusses why the metrics are chosen and defined as they are. \par

\section{Comparison between OCR models}

Both Tesseract and EasyOCR are sophisticated OCR engines and are available as open-source in their default versions. However, there exists a large number of differences in how they have been developed. While Tesseract \cite{Google2015} happens to be a quite superior alternative right from the beginning which can use even PDF files as input apart from the standard images, it turns out that there doesn’t exist any suitable interface with common programming languages (such as Python) which leads to a lot of problems. The first and foremost problem is that Tesseract needs to be set up in a way similar to how a particular program is set up on a remote server i.e. the only link between a program using Tesseract and the actual engine is the file path. Customisations can be done nonetheless, but are difficult to implement due to the lack of a high-level interface. This leads directly to another issue: the Ezmeral MLOPs platform is set up on a legacy Centos 7.5 virtual machine cluster and won’t allow arbitrary software to be installed in it due to the presence of an enterprise-grade security architecture.  So it may be the better OCR engine in general, however, this use case is unsuitable for it.\par

EasyOCR offers as many capabilities \cite{Jaided2020} as Tesseract (sans the direct processing of PDF files) and has a high-level library directly available for use in Python. This means that no preliminary server like set-up needs to be done to access the OCR engine, rather everything is on board once the program and all its imported modules are loaded onto memory. Hence, EasyOCR was chosen as the OCR for this project. \par

It should be noted that EasyOCR is a ready-to-use OCR that is invariant to colour images while the Tesseract documentation does say things about providing a greyscale image as input. This feature alone avoided a lot of extra image processing operations within the codebase. After using EasyOCR, the following changes took place:

\begin{itemize}
 \item The size of the codebase was greatly reduced.
 \item The execution time dropped significantly.
 \item Major improvements were not visible in accuracy although EasyOCR still managed to perform better.
\end{itemize}

Amongst all decoded values at the end of a particular run, the following results were obtained and are summarised below.

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.7]{chapter5/barchart.png}
  \caption{Comparison of Tesseract and EasyOCR for a sample $30$ minutes telecast}
  \label{fig:easy_vs_tess}
\end{figure}

Still, EasyOCR (and the project) benefits even further from a possible change in hardware configuration which is detailed in the following section.


\section{Comparison between hardware architectures}


\section{Accuracy, empirical accuracy and execution time}

A lot has been discussed about the various performance metrics used to evaluate object detection models in section \ref{metrics}, however, we still stick to the concept of simple empirical accuracy in the above two sections. The reason for the same is that there are multiple ML/DL models which are involved in this project. Each model has been designed (or is being used) to complete only a specific task amongst a large number of tasks that are to be executed sequentially. Each task involves different kinds of data and hence produces different kinds of output. \par

A simple e.g. being that the YoLo models discussed throughout the report output a set of mixed values as given in section \ref{brief_discuss} after taking in an image as an input. However, the OCR models that follow require an image as an input, but produce a textual output (i.e. of string datatype). These models within them have two deep learning models i.e. a text detection and a language model in a cascaded fashion out of which only the former receives an input in the form of an image while the second model deals with textual input and output. \par

Each of these models named above has its own set of accuracy metrics which may be discussed however, they would be irrelevant to the final output of the project. Hence, we have stuck to the concept of simple empirical accuracy which is defined by the set of values obtained at the end from the OCR and summarised in the CSV file and is defined as follows

\begin{equation}
\centering
A_{e} = \frac{No. \ of  \ NA \ values \ in \ the \ CSV}{Total \ no. \ of \ values \ in \ the \ CSV }

\end{equation}

Measuring execution time remains the same i.e. in seconds although at a couple of places wherein the execution time is large, seconds have been replaced by larger units of time.
\section{Future work}

The current system can be improvised in two different areas as given below.

\subsection{Accuracy and efficiency}

The current system does filtering at only the FPS rate which manages to reduce the number of frames under consideration to a much smaller value (a $25$\textsuperscript{th} of it to be precise). However, as we have seen in chapter \ref{chapter3}, the frame selection is still not inherently intelligent and still leads to the production of  $ \approx 1400$ frames on an average, amongst which frames would be manually selected for annotation. \par

Theoretically, we can say that for the problem statement at hand; there exists a program $P$ that extracts and processes exactly $N$ frames such that the total no. of recommendations being made is $R \leqslant N$. \par

To do so we can use an adaptive frame extraction algorithm \cite{adapt} which shall extract only the frames which are important to us. This method is based on matching histograms of successive frames, a correlation that exists amongst their colour channels, etc. This shall greatly reduce the number of frames to be processed at a time (apart from the already existing FPS rate filtering). This method, however, could be tending to fail because similar-looking frames need not be redundant to us just because they appear close to each other in the actual video. In that case, we can go for an alternative approach wherein the frame numbers are output according to a simple implementation of any multi-output regressor \cite{ Brownlee2020}. Since there could be inherent inaccuracies in the training of the same, we can use a spreading factor $\delta$ which shall extract keyframes starting from ${f}_{k} - \delta$ to ${f}_{k} + \delta$ instead of just the frame $f_k$.

\subsection{Usability}

Currently, a two-part deployment and its necessity are deemed to be confusing to many SEBI officers who work in the surveillance department and don’t have any prior programming knowledge. Both these deployments could be integrated into a single native cross-platform desktop app developed either using \href{https://riverbankcomputing.com/software/pyqt/intro}{PyQt} or \href{electronjs.org}{Electron} which shall have an intuitive front end design and the same set of calling scripts as its backend. This shall help concerned officers keep track of ongoing workflows as well as use them when required without interfering with ongoing batch workflows and being aware of the complexity of the ML/DL code behind it.
